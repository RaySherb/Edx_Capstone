---
title: "Movie Recommendation System"
author: "Ray Sherbourne"
date: "March 13, 2021"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Project Overview
###section that describes the dataset and summarizes the goal of the project and key steps that were performed

The goal of this project is to solve a popular data science problem known as **The Netflix Challenge**. A dataset of movies and user ratings is provided that will serve as the foundation to build a recommendation system. A validation set is extracted from the data to simulate a true unknown sample and to serve as a final validation test set. The accuracy, as measured against the validation set, is specified as the root mean squared error (RMSE), with a target of 0.86499 or better.

The key steps that are to be performed are:
1. Import data
  + Download data from source
  + Extract validation set
2. Data wrangling to clean the data for ease of use
3. Exploratory analysis
4. Model building
  + Start with the mean as a baseline approach
  + Introduce a movie bias term
  + Introduce a user bias term
  + Introduce a regularization term lambda
  + Use matrix factorization (with and without optimizing parameters)
5. Train with best performing model and test on validation set
6. Reflect on the results

#Method
###section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach

The data was downloaded from the grouplens website and 10% was immediately set aside as the validation set. 

```{r, echo=FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
#movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#title = as.character(title),
#genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)


```


Looking at the test set, there appears to be two useful data munging operations we can do immediately:
* The timestamp predictor appears as a common format of seconds since midnight (UTC) January 1st, 1970. Although this is a common format, it is un-intuitive to work with so this is converted to a date time format. 
```{r, echo=FALSE}
# Converts timestamp to use-able date of review format
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
library(lubridate)
edx <- mutate(edx, date = as_datetime(timestamp))
```
* The release year may have some value to our exploration so it is extracted from the movie title.
```{r, echo=FALSE}
# Extracts release year
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
library(stringr)
pattern <- "\\(\\d{4}\\)"
pattern2 <- "\\d{4}"
edx <- mutate(edx, release=str_extract(edx$title, pattern))#%>%
edx <- edx %>% mutate(release=as.numeric(str_extract(edx$release, pattern2)))
```

Let's now visualize each of our predictors, to see if there are any apparent patterns or a hypothesis that  merit further investigation.

```{r, echo=FALSE}
# User ratings
edx %>% group_by(userId) %>% summarise(n = n()) %>% 
  ggplot(aes(n)) + geom_histogram() + scale_x_continuous(trans='log10')
# Movie ratings
edx %>% group_by(movieId) %>% summarise(n = n()) %>% 
  ggplot(aes(n)) + geom_histogram() + scale_x_continuous(trans='log10')
# Ratings
edx %>% group_by(rating) %>% summarise(n = n()) %>%  
  ggplot(aes(x=rating, y=n)) + geom_line()
# Timestamp
edx %>% mutate(date=round_date(date, unit='week')) %>% group_by(date) %>%
  summarise(rating=mean(rating)) %>%
  ggplot(aes(x=date, y=rating)) + geom_point() + geom_smooth()
```







Before we start building our first model, we need to split the data into a training and test set. (Reminder that the validation set replicates data that we don't have yet)
```{r, echo=FALSE}
##########################################################
# Model Building / Select Machine Learning Algorithm
##########################################################

# Split the training set into training/test sets to design/test models
set.seed(420, sample.kind = 'Rounding')
test_index <- createDataPartition(y=edx$rating, times = 1, p=0.1, list=F)
edx_test <- edx[test_index,]
edx_train <- edx[-test_index,]

# This ensures we don't test on movies/users we have never seen before
edx_test <- edx_test %>% semi_join(edx_train, by='movieId') %>%
  semi_join(edx_train, by='userId')
rm(test_index)
```
We will use the simplest approach as a benchmark and iteratively improve upon it. Therefore, our first model will predict the average movie rating for all movies. 
```{r, echo=FALSE}
# This model predicts the avg movie rating for all cases
mu <- edx_train$rating %>% mean()
(naive_rmse <- RMSE(edx_test$rating, mu))
```
To account for the fact that movies are not received by audiences equally, we can introducing a bias term. This first bias term will represent how a particular movie deviates from the average of all movies.
```{r, echo=FALSE}
# This model builds on previous by introducing a movie bias term
movie_avgs <- edx_train %>% group_by(movieId) %>% summarise(b_i=mean(rating-mu))
movie_bias_model <- mu + edx_test %>% left_join(movie_avgs, by='movieId') %>% pull(b_i)
(movie_bias_rmse <- RMSE(edx_test$rating, movie_bias_model))
```
Likewise, different users have different movie critiquing tendencies. We can introduce a bias term for users that will represent how a particular user deviates from the average user in rating movies.
```{r, echo=FALSE}
# This model builds on previous by introducing a user bias term
user_avgs <- edx_train %>% left_join(movie_avgs, by='movieId') %>% group_by(userId) %>% summarise(b_u=mean(rating-mu-b_i))
user_movie_bias_model <- edx_test %>% left_join(movie_avgs, by='movieId') %>% left_join(user_avgs, by='userId') %>%
  mutate(pred=mu+b_i+b_u) %>% pull(pred)
(user_movie_bias_rmse <- RMSE(edx_test$rating, user_movie_bias_model))
```
Now, let's see how the results would change if we reversed the order of the calculation of our bias terms.
```{r, echo=FALSE}
user_avgs <- edx_train %>% group_by(userId) %>% summarise(b_u=mean(rating-mu))
user_bias_model <- mu + edx_test %>% left_join(user_avgs, by='userId') %>% pull(b_u)
(user_bias_rmse <- RMSE(edx_test$rating, user_bias_model))

# This model builds on previous by introducing a movie bias term
movie_avgs <- edx_train %>% left_join(user_avgs, by='userId') %>% group_by(movieId) %>% summarise(b_i=mean(rating-mu-b_u))
movie_user_bias_model <- edx_test %>% left_join(movie_avgs, by='movieId') %>% left_join(user_avgs, by='userId') %>%
  mutate(pred=mu+b_i+b_u) %>% pull(pred)
(movie_user_bias_rmse <- RMSE(edx_test$rating, movie_user_bias_model))
```
Taking the best form of our biased model, we can introduce a regularization term, lambda, to combat over-fitting. In the context of the Netflix challenge, over-fitting could occur due to a rarely watched movie being incorrectly labeled as exceptionally good and leading to unhelpful recommendations. We will use cross-validation to search for the optimal lambda term.
```{r, echo=FALSE}
# This model builds on biased model by introducing a regularization (lambda) term 

# Use cross-validation to search for best lambda term:
#lambdas <- seq(0, 10, 0.25)
lambdas <- seq(4, 5, 0.1)
rmses <- sapply(lambdas, function(l){
  mu <- mean(edx_train$rating)
  b_i <- edx_train %>% group_by(movieId) %>% summarise(b_i=sum(rating-mu)/(n()+l))
  
  b_u <- edx_train %>% left_join(b_i, by='movieId') %>%
    group_by(userId) %>% summarise(b_u=sum(rating-b_i-mu)/(n()+l))
  
  predicted_ratings <- edx_test %>% left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>% mutate(pred=mu+b_i+b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, edx_test$rating))
})
# visualize the search for best lambda
qplot(lambdas, rmses)
# save the best lambda term
(lambda <- lambdas[which.min(rmses)])

#Regularized movie bias term
b_i <- edx_train %>% group_by(movieId) %>% summarise(b_i=sum(rating-mu)/(n()+lambda))
#Regularized user bias term
b_u <- edx_train %>% left_join(b_i, by='movieId') %>%
  group_by(userId) %>% summarise(b_u=sum(rating-b_i-mu)/(n()+lambda))

regularized_user_movie_model <- edx_test %>% left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>% mutate(pred=mu+b_i+b_u) %>% pull(pred)
(regularized_user_movie_rmse <- RMSE(edx_test$rating, regularized_user_movie_model))

```
We will now try an alternate approach using matrix factorization. The idea behind this approach is that if we arrange the data into a matrix of users x movies, with the intersections representing an individual rating, then we can solve for the matrix of user-preferences and the matrix of movie characteristics, that when multiplied, give the completed matrix which we have just formed. Another way of explaining it is that we find similarities between movies and similarities between users. Then we can recommend movies that similar users liked, or we can recommend a movie that is similar to others that a user has liked.
The Recosystem package was built specifically to handle this type of matrix factorization. Recosystem takes three vectors: one for the users, one for the movies, and one for the ratings. We will include 2 approaches one without tuning the model parameters, and one after tuning the parameters. In tuning the parameters, multiple values were experimented with, however this process took a significant amount of computational time, so only these two models are included here.
```{r, echo=FALSE}

#This model will use matrix factorization without tuning parameters
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")
set.seed(69, sample.kind = 'Rounding')

train_set <- with(edx_train, data_memory(user_index=userId, item_index=movieId, rating=rating))
test_set <- with(edx_test, data_memory(user_index=userId, item_index=movieId, rating=rating))

# Create a model object
r <- Reco()

#train with tuned parameters
r$train(train_set)

#predict
matrix_factor <- r$predict(test_set, out_memory())

(matrix_factor_rmse <- RMSE(matrix_factor, edx_test$rating))


#This model will tune the training parameters

# Call the $tune() method to select the best parameters with cross validation 
# (This will take a while, see progress bar in R console)
opts <- r$tune(train_set, opts = list(dim=c(10, 20, 30), #dim is number of latent factors
                                      lrate=c(0.1, 0.2), #learning rate (step size in gradient decent)
                                      costp_l1=0, costq_l1=0, #L1 regularization terms set to 0 (default is c(0.01, 0.1))
                                      nthread=1, #number of threads for parallel computing
                                      niter=10)) #number of iterations
opts # $min shows the parameters that minimize the loss function, $res shows the loss function value of each combo

#train with tuned parameters
r$train(train_set, opts=c(opts$min, nthread=1, niter=20))

#predict
matrix_factor_tuned <- r$predict(test_set, out_memory())

(matrix_factor_tuned_rmse <- RMSE(matrix_factor_tuned, edx_test$rating))

```

```{r, echo=FALSE}
# Evaluate the different models
methods <- c('Just the average', '+ movie bias', '+ user bias', 'reversed_biases', 'regularized model', 'matrix factorization', 'matrix factorization (Tuned)')
rmses <- c(naive_rmse, movie_bias_rmse, user_movie_bias_rmse, movie_user_bias_rmse, regularized_user_movie_rmse, matrix_factor_rmse, matrix_factor_tuned_rmse)
(model_evaluations <- tibble(method=methods, RMSE=rmses))
```
#Results
###section that presents the modeling results and discusses the model performance
We now have several models, and we will select the best performing one We will now run our best performing model on the validation set
```{r, echo=FALSE}

```



#Conclusion
###section that gives a brief summary of the report, its limitations and future work