---
title: "Movie Recommendation System"
author: "Ray Sherbourne"
date: "March 15, 2021"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project Overview

In 2006, Netflix started an open competion to build a model that would predict user ratings for movies using only past ratings as the only predictors. This competition has become a popular machine learning challenge in the data science community. This report will provide one solution to a modified form of this problem, known as **The Netflix Challenge**, using a dataset provided by grouplens.org. The accuracy of the models presented will be measured with the root mean squared error (RMSE), with a target of 0.8775 or lower.

The key steps to be performed are:
1. Import data
  + Download data from source
  + Extract validation set
2. Data wrangling to clean the data for ease of use
3. Exploratory analysis
4. Model building
  + Start with the mean as a baseline approach
  + Introduce a movie bias term
  + Introduce a user bias term
  + Introduce a regularization term lambda
  + Use matrix factorization (with and without optimizing parameters)
5. Train with best performing model and test on validation set
6. Reflect on the results

# Method

The data was downloaded as a zip file from the grouplens website. Two files (movies.dat & ratings.dat) were extracted and joined together by movieId into a dataframe containing columns for userId, movieId, rating, timestamp, title, and genres. With the data collection complete, 10% was immediately set aside as the validation set. 

```{r, include =FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
#movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#title = as.character(title),
#genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)


```

Two final data munging operations are done on the training set before data exploration:
* The timestamp predictor appears as a common format of seconds since midnight (UTC) January 1st, 1970. Although this is a common format, it is un-intuitive to work with so this is converted to a date time format. 
```{r, include=FALSE}
# Converts timestamp to use-able date of review format
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
library(lubridate)
edx <- mutate(edx, date = as_datetime(timestamp))
```
* The release year may have some value to our exploration so it is extracted from the movie title.
```{r, include=FALSE}
# Extracts release year
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
library(stringr)
pattern <- "\\(\\d{4}\\)"
pattern2 <- "\\d{4}"
edx <- mutate(edx, release=str_extract(edx$title, pattern))#%>%
edx <- edx %>% mutate(release=as.numeric(str_extract(edx$release, pattern2)))
```

Visual summaries are often useful in conveying and understanding data. Simply plotting the predictors will serve as the primary method of data exploration. The following histogram shows the number of users that rate a given amount of movies. 

```{r, echo=FALSE, message=FALSE}
# User ratings
edx %>% group_by(userId) %>% summarise(n = n()) %>% 
  ggplot(aes(n)) + geom_histogram() + scale_x_continuous(trans='log10')
```

The next histogram shows the number of movies that have a given amount of reviews. There are a significant amount of movies that have been rated less than 10 times. This could potentially be a source of error that may need to be solved with regularization.

```{r, echo=FALSE, message=FALSE}
# Movie ratings
edx %>% group_by(movieId) %>% summarise(n = n()) %>% 
  ggplot(aes(n)) + geom_histogram() + scale_x_continuous(trans='log10')
```

The next plot is a line graph showing the frequency of each possible rating. Whole number ratings are much more common.

```{r, echo=FALSE, message=FALSE}
# Ratings
edx %>% group_by(rating) %>% summarise(n = n()) %>%  
  ggplot(aes(x=rating, y=n)) + geom_line()
```

The next plot shows the mean rating for all movies based on the week the review was given. This shows that time-series analysis may have some predictive power, but it is relatively small.

```{r, echo=FALSE, message=FALSE}
# Timestamp
edx %>% mutate(date=round_date(date, unit='week')) %>% group_by(date) %>%
  summarise(rating=mean(rating)) %>%
  ggplot(aes(x=date, y=rating)) + geom_point() + geom_smooth()
```





After splitting the data into a training and test set (Reminder that the validation set remains untouched), the process of building/testing models begins. For this approach, instead of starting with a complex model, a simple model will be built and itteratively improved, while serving as a benchmark to gauge progress.
```{r, echo=FALSE, warning=FALSE}
##########################################################
# Model Building / Select Machine Learning Algorithm
##########################################################

# Split the training set into training/test sets to design/test models
set.seed(420, sample.kind = 'Rounding')
test_index <- createDataPartition(y=edx$rating, times = 1, p=0.1, list=F)
edx_test <- edx[test_index,]
edx_train <- edx[-test_index,]

# This ensures we don't test on movies/users we have never seen before
edx_test <- edx_test %>% semi_join(edx_train, by='movieId') %>%
  semi_join(edx_train, by='userId')
rm(test_index)
```
The first model will simply predict the average movie rating for all cases:
```{r, echo=FALSE}
# This model predicts the avg movie rating for all cases
mu <- edx_train$rating %>% mean()
naive_rmse <- RMSE(edx_test$rating, mu)
```

Model                                     |    RMSE
------------------------------------------|-------------
Average                                   | `naive_rmse`


To improve this model, it is possible to account for the fact that movies are not received by audiences equally. A bias term is calculated by taking the average of the difference between the individual ratings of a movie and the average of all movie ratings. This will represent how a particular movie deviates from the average movie, and will be added onto our previous model: 
```{r, echo=FALSE, message=FALSE}
# This model builds on previous by introducing a movie bias term
movie_avgs <- edx_train %>% group_by(movieId) %>% summarise(b_i=mean(rating-mu))
movie_bias_model <- mu + edx_test %>% left_join(movie_avgs, by='movieId') %>% pull(b_i)
movie_bias_rmse <- RMSE(edx_test$rating, movie_bias_model)
```
Model                                     |    RMSE
------------------------------------------|-------------
Average                                   | `naive_rmse`
Avg + Movie Bias                          | `movie_bias_rmse`


Likewise, different users have different movie critiquing tendencies. A bias term for users, is calculated by taking the average of each users rating after subtracting the average movie rating and movie bias term. The user bias term will account for how a particular movie deviates from the average movie for a particular user. Again this term is added onto the previous model:
```{r, echo=FALSE, message=FALSE}
# This model builds on previous by introducing a user bias term
user_avgs <- edx_train %>% left_join(movie_avgs, by='movieId') %>% group_by(userId) %>% summarise(b_u=mean(rating-mu-b_i))
user_movie_bias_model <- edx_test %>% left_join(movie_avgs, by='movieId') %>% left_join(user_avgs, by='userId') %>%
  mutate(pred=mu+b_i+b_u) %>% pull(pred)
user_movie_bias_rmse <- RMSE(edx_test$rating, user_movie_bias_model)
```
Model                                     |    RMSE
------------------------------------------|-------------
Average                                   | `naive_rmse`
Avg + Movie Bias                          | `movie_bias_rmse`
Avg + Movie Bias + User Bias              | `user_movie_bias_rmse`

The formula for the second bias term included the first bias term, so a quick check is done to see the results of switching the order of calculation:
```{r, echo=FALSE, message=FALSE}
user_avgs <- edx_train %>% group_by(userId) %>% summarise(b_u=mean(rating-mu))
user_bias_model <- mu + edx_test %>% left_join(user_avgs, by='userId') %>% pull(b_u)
user_bias_rmse <- RMSE(edx_test$rating, user_bias_model)

# This model builds on previous by introducing a movie bias term
movie_avgs <- edx_train %>% left_join(user_avgs, by='userId') %>% group_by(movieId) %>% summarise(b_i=mean(rating-mu-b_u))
movie_user_bias_model <- edx_test %>% left_join(movie_avgs, by='movieId') %>% left_join(user_avgs, by='userId') %>%
  mutate(pred=mu+b_i+b_u) %>% pull(pred)
movie_user_bias_rmse <- RMSE(edx_test$rating, movie_user_bias_model)
```
Model                                     |    RMSE
------------------------------------------|-------------
Average                                   | `naive_rmse`
Avg + Movie Bias                          | `movie_bias_rmse`
Avg + Movie Bias + User Bias              | `user_movie_bias_rmse`
Avg + User Bias                           | `user_bias_rmse`
Avg + User Bias + Movie Bias              | `movie_user_bias_rmse`

The best form of the model above will be itterated further by adding a regularization term, lambda, to combat over-fitting. In the context of The Netflix Challenge, over-fitting could occur due to a rarely watched movie being incorrectly labeled as exceptionally good and leading to unhelpful recommendations. Cross-validation is used to search for the optimal lambda term.
```{r, echo=FALSE, message=FALSE}
# This model builds on biased model by introducing a regularization (lambda) term 

# Use cross-validation to search for best lambda term:
#lambdas <- seq(0, 10, 0.25)
lambdas <- seq(4, 5, 0.1)
rmses <- sapply(lambdas, function(l){
  mu <- mean(edx_train$rating)
  b_i <- edx_train %>% group_by(movieId) %>% summarise(b_i=sum(rating-mu)/(n()+l))
  
  b_u <- edx_train %>% left_join(b_i, by='movieId') %>%
    group_by(userId) %>% summarise(b_u=sum(rating-b_i-mu)/(n()+l))
  
  predicted_ratings <- edx_test %>% left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>% mutate(pred=mu+b_i+b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, edx_test$rating))
})
# visualize the search for best lambda
qplot(lambdas, rmses)
# save the best lambda term
(lambda <- lambdas[which.min(rmses)])

#Regularized movie bias term
b_i <- edx_train %>% group_by(movieId) %>% summarise(b_i=sum(rating-mu)/(n()+lambda))
#Regularized user bias term
b_u <- edx_train %>% left_join(b_i, by='movieId') %>%
  group_by(userId) %>% summarise(b_u=sum(rating-b_i-mu)/(n()+lambda))

regularized_user_movie_model <- edx_test %>% left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>% mutate(pred=mu+b_i+b_u) %>% pull(pred)
regularized_user_movie_rmse <- RMSE(edx_test$rating, regularized_user_movie_model)

```
Model                                     |    RMSE
------------------------------------------|-------------
Average                                   | `naive_rmse`
Avg + Movie Bias                          | `movie_bias_rmse`
Avg + Movie Bias + User Bias              | `user_movie_bias_rmse`
Avg + User Bias + Movie Bias              | `movie_user_bias_rmse`
Avg + Movie Bias + User Bias + Lambda     | `regularized_user_movie_rmse`

This model passes the accuracy goal for this project, however there is an alternate approach using matrix factorization. The idea behind this approach is that if the data is arranged into a matrix of users x movies, with the intersections representing an individual rating, then factorization would results in a matrix of user-preferences and the matrix of movie characteristics. It would be possible to group similar users and group similar movies, and then make recommendations based on these groups.
The Recosystem package was built specifically to handle this type of matrix factorization. Recosystem takes three vectors: one for the users, one for the movies, and one for the ratings. This report includes two approaches: one without tuning the model parameters, and one after tuning the parameters. In tuning the parameters, multiple values were experimented with, however this process took a significant amount of computational time, so only these two models are included here.
```{r, echo=FALSE, message=FALSE, warning=FALSE}

#This model will use matrix factorization without tuning parameters
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")
set.seed(69, sample.kind = 'Rounding')

train_set <- with(edx_train, data_memory(user_index=userId, item_index=movieId, rating=rating))
test_set <- with(edx_test, data_memory(user_index=userId, item_index=movieId, rating=rating))

# Create a model object
r <- Reco()

#train with tuned parameters
r$train(train_set)

#predict
matrix_factor <- r$predict(test_set, out_memory())

(matrix_factor_rmse <- RMSE(matrix_factor, edx_test$rating))


#This model will tune the training parameters

# Call the $tune() method to select the best parameters with cross validation 
# (This will take a while, see progress bar in R console)
opts <- r$tune(train_set, opts = list(dim=c(10, 20, 30), #dim is number of latent factors
                                      lrate=c(0.1, 0.2), #learning rate (step size in gradient decent)
                                      costp_l1=0, costq_l1=0, #L1 regularization terms set to 0 (default is c(0.01, 0.1))
                                      nthread=1, #number of threads for parallel computing
                                      niter=10)) #number of iterations
opts # $min shows the parameters that minimize the loss function, $res shows the loss function value of each combo

#train with tuned parameters
r$train(train_set, opts=c(opts$min, nthread=1, niter=20))

#predict
matrix_factor_tuned <- r$predict(test_set, out_memory())

(matrix_factor_tuned_rmse <- RMSE(matrix_factor_tuned, edx_test$rating))

```

Model                                     |    RMSE
------------------------------------------|-------------
Average                                   | `naive_rmse`
Avg + Movie Bias                          | `movie_bias_rmse`
Avg + Movie Bias + User Bias              | `user_movie_bias_rmse`
Avg + User Bias + Movie Bias              | `movie_user_bias_rmse`
Avg + Movie Bias + User Bias + Lambda     | `regularized_user_movie_rmse`
Matrix Factorization (Untuned)            | `matrix_factor_rmse`
Matrix Factorization (Tuned)              | `matrix_factor_tuned_rmse`



#Results
###section that presents the modeling results and discusses the model performance
We now have several models, and we will select the best performing one We will now run our best performing model on the validation set
```{r, echo=FALSE}

```



#Conclusion
###section that gives a brief summary of the report, its limitations and future work